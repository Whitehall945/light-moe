# Light-MoE

<p align="center">
  <b>High-Performance Pipeline-Parallel MoE Inference Engine with CuTe</b>
</p>

<p align="center">
  <a href="#features">Features</a> â€¢
  <a href="#installation">Installation</a> â€¢
  <a href="#quick-start">Quick Start</a> â€¢
  <a href="#architecture">Architecture</a> â€¢
  <a href="#benchmarks">Benchmarks</a> â€¢
  <a href="README_CN.md">ä¸­æ–‡æ–‡æ¡£</a>
</p>

---

## Overview

**Light-MoE** is a high-performance distributed inference engine specifically designed for Mixture-of-Experts (MoE) models. It addresses the unique challenges of MoE inference: dynamic load imbalance and All-to-All communication overhead.

Unlike general-purpose frameworks like vLLM that focus on KV cache management, Light-MoE targets the **expert bottleneck** through:
- Custom CuTe-based operators for maximum hardware utilization
- Asynchronous communication-computation overlap
- Dynamic token dispatching with load balancing

## Features

### Operator Level (CuTe/CUTLASS)
- ğŸ”¥ **Fused MoE Gate + TopK**: Reduces global memory access through kernel fusion
- âš¡ **Grouped GEMM**: Dynamic shape support for expert computation with optimized Tensor Core utilization
- ğŸ¯ **W4A16 Quantized GEMM**: Weight-only INT4 quantization for memory-constrained environments

### Infrastructure Level
- ğŸŒ **Expert Parallelism (EP)**: Flexible expert placement across GPUs
- ğŸ”„ **Communication-Computation Overlap**: Hide All-to-All latency with pipelined execution
- âš–ï¸ **Dynamic Load Balancing**: Real-time workload distribution across experts

## Installation

### Prerequisites
- CUDA 12.0+
- Python 3.10+
- PyTorch 2.0+
- NCCL 2.18+

### From Source
```bash
git clone https://github.com/YOUR_USERNAME/light-moe.git
cd light-moe
git submodule update --init --recursive

# Install Python package
pip install -e .

# Or build C++ library only
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
make -j$(nproc)
```

## Quick Start

```python
from light_moe import LightMoEEngine

# Initialize engine with 8 GPUs
engine = LightMoEEngine(
    model_path="path/to/mixtral-8x7b",
    tensor_parallel_size=1,
    expert_parallel_size=8,
)

# Run inference
output = engine.generate(
    prompt="Explain the theory of relativity",
    max_tokens=512,
)
print(output)
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Light-MoE Engine                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Python Frontend   â”‚              C++/CUDA Core                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Scheduler        â”‚  â€¢ CuTe Operators (Grouped GEMM, Gate)    â”‚
â”‚  â€¢ Model Loader     â”‚  â€¢ NCCL Communication Layer               â”‚
â”‚  â€¢ API Server       â”‚  â€¢ Async Dispatcher                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Directory Structure
```
light-moe/
â”œâ”€â”€ include/          # C++ public headers
â”œâ”€â”€ src/              # C++/CUDA source code
â”‚   â”œâ”€â”€ ops/cute/     # CuTe-based operators
â”‚   â””â”€â”€ comm/         # Communication layer
â”œâ”€â”€ python/           # Python bindings and frontend
â”œâ”€â”€ tests/            # Unit and integration tests
â”œâ”€â”€ benchmarks/       # Performance benchmarks
â””â”€â”€ docs/             # Documentation
```

## Benchmarks

| Configuration | Model | Throughput (tokens/s) | Improvement |
|--------------|-------|----------------------|-------------|
| 8x 2080 Ti   | Mixtral-8x7B | TBD | TBD vs baseline |

## Roadmap

- [ ] Phase 1: Core CuTe operators (Grouped GEMM, Fused Gate)
- [ ] Phase 2: Distributed infrastructure (EP, All-to-All)
- [ ] Phase 3: End-to-end integration and benchmarking

## Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## License

This project is licensed under the Apache License 2.0 - see [LICENSE](LICENSE) for details.

## Acknowledgments

- [NVIDIA CUTLASS](https://github.com/NVIDIA/cutlass) for the CuTe framework
- [vLLM](https://github.com/vllm-project/vllm) for inspiration on inference engine design
